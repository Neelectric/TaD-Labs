{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN/GCcbgNQdqeQ1AFmctq+f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neelectric/TaD-Labs/blob/main/Text_as_Data_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text as Data Coursework\n",
        "by Neel Rajani, 2514211r \n",
        "\n",
        "*   Note: While answering questions 3-5, I was at times very insecure of whether my code was doing what the assignment was asking for. In addition to asking friends for how they understood questions, I occasionally turned to chatGPT for advice and demonstrations of how some functions work or how to plot things. At times I adapted some of those demonstrations, and at all times rewrote the code shown to me. For example, it suggested hex values of pastel colours to me, or showed me how to print evaluation metrics nicely with pandas dataframs. In complying with University plagiarism rules, I hereby give due credit to chatGPT for supporting my work."
      ],
      "metadata": {
        "id": "ghP4cuPfYeHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fix due to this stackoverflow post. https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "print(locale.getpreferredencoding())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRgmVUmf8Wor",
        "outputId": "d5ca83d8-e5c7-4c97-c0ee-f257a78c6b1f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "YwMVqBdU8d18"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#small cell due to https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results that disables word wrap\n",
        "#enabled during report write up to make taking screenshots easier\n",
        "\n",
        "# from IPython.display import HTML, display\n",
        "\n",
        "# def set_css():\n",
        "#   display(HTML('''\n",
        "#   <style>\n",
        "#     pre {\n",
        "#         white-space: pre-wrap;\n",
        "#     }\n",
        "#   </style>\n",
        "#   '''))\n",
        "# get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "#small snippet for pandas to show all columns and rows of dataframes with unlimited width\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "metadata": {
        "id": "ycoO6Gn1HTxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1ea35e-b94f-491c-9243-a2dcd2ef03f0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-64-6354ad2bbf2f>:20: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  pd.set_option('display.max_colwidth', -1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import statements\n",
        "import json\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4uRYGe4n7dQ0"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tdr2SZL36mWr",
        "outputId": "6667c500-1109-4d8e-dafd-d8fa65a2c056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-12 19:27:58--  https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EY_R8Y7DkrxMqXGe-zlgeNkBdJU5ZNTf8FYrN2pqDwddMA?download=1\n",
            "Resolving gla-my.sharepoint.com (gla-my.sharepoint.com)... 13.107.136.8, 13.107.138.8, 2620:1ec:8f8::8, ...\n",
            "Connecting to gla-my.sharepoint.com (gla-my.sharepoint.com)|13.107.136.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://gla-my.sharepoint.com/personal/jake_lever_glasgow_ac_uk/_layouts/15/download.aspx?UniqueId=8ef1d18f92c34cbca9719efb396078d9 [following]\n",
            "--2023-03-12 19:27:58--  https://gla-my.sharepoint.com/personal/jake_lever_glasgow_ac_uk/_layouts/15/download.aspx?UniqueId=8ef1d18f92c34cbca9719efb396078d9\n",
            "Reusing existing connection to gla-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1279064 (1.2M) [application/json]\n",
            "Saving to: ‘reddit_posts.json’\n",
            "\n",
            "reddit_posts.json   100%[===================>]   1.22M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-12 19:27:58 (23.2 MB/s) - ‘reddit_posts.json’ saved [1279064/1279064]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Download data from reddit\n",
        "!wget -O reddit_posts.json https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EY_R8Y7DkrxMqXGe-zlgeNkBdJU5ZNTf8FYrN2pqDwddMA?download=1\n",
        "\n",
        "#load into python\n",
        "with open('reddit_posts.json', 'rt') as file:\n",
        "  source = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 - Dataset\n"
      ],
      "metadata": {
        "id": "gLU6Ciu9YpS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels, texts = [], []\n",
        "texts = [entry['title'] + '\\n' + entry['body'] for entry in source]\n",
        "labels = [entry['subreddit'] for entry in source]\n",
        "Counter(labels).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8D3ZQVqGgJA",
        "outputId": "34f68756-79e0-4ccc-db9f-2a23c3bba4a5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NintendoSwitch', 249),\n",
              " ('tea', 236),\n",
              " ('Coffee', 234),\n",
              " ('PS4', 233),\n",
              " ('antiMLM', 226),\n",
              " ('pcgaming', 225),\n",
              " ('xbox', 213),\n",
              " ('HydroHomies', 210),\n",
              " ('Soda', 174)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_train_val, labels_test, texts_train_val, texts_test = train_test_split(labels, texts, test_size=0.2, random_state=42)\n",
        "labels_train, labels_val, texts_train, texts_val = train_test_split(labels_train_val, texts_train_val, test_size=0.25, random_state=42)\n",
        "print(f\"{len(texts_train)=},{len(labels_train)=}\")\n",
        "print(f\"{len(texts_val)=},{len(labels_val)=}\")\n",
        "print(f\"{len(texts_test)=},{len(labels_test)=}\")\n",
        "total_texts = len(texts_train) + len(texts_val) + len(texts_test)\n",
        "len(texts_train)/total_texts, len(texts_val)/total_texts, len(texts_test)/total_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t234C2fs3V3E",
        "outputId": "c85e0f1f-ab7a-4b2a-f294-a8ab1ffdd48e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(texts_train)=1200,len(labels_train)=1200\n",
            "len(texts_val)=400,len(labels_val)=400\n",
            "len(texts_test)=400,len(labels_test)=400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6, 0.2, 0.2)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function to apply spacy tokenizer on text\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def spacyfy(text):\n",
        "  tokens = []\n",
        "  doc = nlp(text)\n",
        "  for t in doc:\n",
        "    if not t.is_stop and not t.is_punct and not t.is_space:\n",
        "      tokens.append(t.lemma_.lower())\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "p0PNAKbUNVFu"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2: Clustering\n",
        "What follows is a cell by cell execution of k-means clustering, with k hardcoded to 5 and the full \"texts\" list chosen as input, as I want to learn about the clustering of the whole dataset, and not only texts_train"
      ],
      "metadata": {
        "id": "qVygUeqDWYtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_vec_scikitlearn_spacy(input):\n",
        "  vectorizer = TfidfVectorizer(tokenizer = spacyfy)\n",
        "  return vectorizer.fit_transform(input)"
      ],
      "metadata": {
        "id": "-rSYf7qhXjgJ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the aim of getting 5 random centroids to start with, I elected the Forgy partition approach: To randomly select k data points from dataset as initial centroids"
      ],
      "metadata": {
        "id": "8LbpzbjQO7Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_indices = random.sample(range(0, 1199), 5)\n",
        "#every time this cell is called, new random numbers are printed. to have some consistency, I decided to copy this output at some point. the output is hardcoded below\n",
        "random_indices = [1055, 1164, 28, 1136, 618]"
      ],
      "metadata": {
        "id": "h8b3Z-KU7nQg"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 0: Vectorise text\n",
        "k = 5\n",
        "tfidf_matrix = tfidf_vec_scikitlearn_spacy(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhlLCRPu5aot",
        "outputId": "3dd1bd5e-d6fa-4681-b739-cc3f5cd34df4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Pick k random \"centroids\"\n",
        "centroids = [tfidf_matrix[index] for index in random_indices]\n",
        "print(len(centroids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpqpM71YlYSk",
        "outputId": "8a9895f6-7d11-4a29-ba2b-c7535bdad87b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell implements k-means clustering, iterating until the model converges or reaches 100 iterations (which it doesn't)"
      ],
      "metadata": {
        "id": "p1chVK5LPHPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create dictionary to save closest centroid per text, populate with dummy data\n",
        "closest_centroid_per_text = {i: [-1,0] for i, _ in enumerate(texts)}\n",
        "\n",
        "print(\"start\")\n",
        "converged = False\n",
        "iterations = 0\n",
        "while(not converged) & (iterations != 100):\n",
        "  converged = True\n",
        "  entries_by_centroid = [[] for i in range(k)]\n",
        "  print(\"iteration: \" + str(iterations))\n",
        "  for i in range(len(texts)):\n",
        "    #Step 2: Assign each vector to its closest centroid\n",
        "    similarity_list = [centroid.dot(tfidf_matrix[i,:].T)[0,0] for centroid in centroids]\n",
        "    closest_centroid_index = np.argmax(similarity_list)\n",
        "    entries_by_centroid[closest_centroid_index].append(tfidf_matrix[i])\n",
        "\n",
        "    #if the newly calculated closest centroid (and the degree of similarity) differs from the old one, we must update this, and have not yet converged\n",
        "    if (closest_centroid_per_text[i] != [closest_centroid_index, similarity_list[closest_centroid_index]]):\n",
        "      converged = False\n",
        "      closest_centroid_per_text[i] = [closest_centroid_index, similarity_list[closest_centroid_index]]\n",
        "\n",
        "  #Step 3: Recalculate the centroids based on the closest vectors\n",
        "  for i in range(k):\n",
        "      n_matrices = len(entries_by_centroid[i])\n",
        "      m,n = entries_by_centroid[i][0].shape\n",
        "      average_matrix = csr_matrix((m,n), dtype=np.float32)\n",
        "      for matrix in entries_by_centroid[i]:\n",
        "        average_matrix += matrix\n",
        "      average_matrix /= n_matrices\n",
        "      #amend 'centroids' to reflect this\n",
        "      centroids[i] = average_matrix\n",
        "  iterations += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdN4NJNnBwWQ",
        "outputId": "60726514-559b-44fe-f30d-e91d73ce8946"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "iteration: 0\n",
            "iteration: 1\n",
            "iteration: 2\n",
            "iteration: 3\n",
            "iteration: 4\n",
            "iteration: 5\n",
            "iteration: 6\n",
            "iteration: 7\n",
            "iteration: 8\n",
            "iteration: 9\n",
            "iteration: 10\n",
            "iteration: 11\n",
            "iteration: 12\n",
            "iteration: 13\n",
            "iteration: 14\n",
            "iteration: 15\n",
            "iteration: 16\n",
            "iteration: 17\n",
            "iteration: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I create a list with 5 sublists, where every sublist contains every text inserted into that cluster"
      ],
      "metadata": {
        "id": "y2kpXYBoOvmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts_by_cluster = []\n",
        "for i in range(k):\n",
        "  cluster_size = 0\n",
        "  cluster_list = []\n",
        "  for j in range(len(closest_centroid_per_text)):\n",
        "    if closest_centroid_per_text[j][0] == i:\n",
        "      cluster_size += 1\n",
        "      cluster_list.append(j)\n",
        "  print(cluster_size)\n",
        "  texts_by_cluster.append(cluster_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy12OcQxqU37",
        "outputId": "57fda280-0f3c-4bf5-e8f3-d47ba3189119"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "566\n",
            "225\n",
            "442\n",
            "444\n",
            "323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing every cluster: an example text, the most represented subreddits, and the top 5 tokens with counts"
      ],
      "metadata": {
        "id": "SM2R34BxOoZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_counters = []\n",
        "for i in range(len(texts_by_cluster)):\n",
        "  cluster_dict = {}\n",
        "  counter = Counter()\n",
        "  \n",
        "  print(\"Cluster \" + str(i+1))\n",
        "  print(\"Example text:\")\n",
        "  print(texts[texts_by_cluster[i][0]][0:200])\n",
        "  for j in range(len(texts_by_cluster[i])):\n",
        "    text_string = spacyfy(texts[texts_by_cluster[i][j]])\n",
        "    counts = Counter(text_string)\n",
        "    counter += counts\n",
        "    if source[texts_by_cluster[i][j]]['subreddit'] in cluster_dict.keys():\n",
        "      cluster_dict[source[texts_by_cluster[i][j]]['subreddit']] +=1\n",
        "    else:\n",
        "      cluster_dict[source[texts_by_cluster[i][j]]['subreddit']] = 1\n",
        "  size = 0\n",
        "  for subreddit, count in cluster_dict.items():\n",
        "    if (count > 10):\n",
        "      print(f\"{subreddit}: {count}\")\n",
        "    size+= count\n",
        "  cluster_counters.append(cluster_dict)\n",
        "  print(\"total size: \"+str(size))\n",
        "  print(\"counts:\")\n",
        "  print(counter.most_common(5))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEMHdiICveia",
        "outputId": "3e43c698-3ee9-4c72-d173-ac66deeeef58"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 1\n",
            "Example text:\n",
            "Please help me find this game!\n",
            "Like 10 or so years ago I loved this game and I can’t remember it now.. \n",
            "It was a game show battle Royale thing where all the  contestants killed each other for a prize.\n",
            "xbox: 45\n",
            "pcgaming: 165\n",
            "PS4: 161\n",
            "NintendoSwitch: 192\n",
            "total size: 566\n",
            "counts:\n",
            "[('game', 1210), ('play', 512), ('like', 307), ('switch', 288), ('look', 172)]\n",
            "\n",
            "\n",
            "Cluster 2\n",
            "Example text:\n",
            "Anyone tried Irn Bru?\n",
            "It’s a Scottish drink and it’s banned some countries and I was wondering if anyone here has tried it. It has quite a unique taste and it’s not something you’d forget quickly. You\n",
            "Soda: 158\n",
            "PS4: 17\n",
            "HydroHomies: 15\n",
            "total size: 225\n",
            "counts:\n",
            "[('soda', 227), ('find', 111), ('like', 89), ('know', 77), ('try', 71)]\n",
            "\n",
            "\n",
            "Cluster 3\n",
            "Example text:\n",
            "I once had a box of tea that I believe was Scottish Highland black tea. Can anyone recommend me a tea along those lines?\n",
            "I'd like to repurchase this tea but I cannot find the exact box and don't want \n",
            "tea: 222\n",
            "Coffee: 203\n",
            "total size: 442\n",
            "counts:\n",
            "[('tea', 780), ('coffee', 470), ('like', 255), ('good', 209), ('try', 180)]\n",
            "\n",
            "\n",
            "Cluster 4\n",
            "Example text:\n",
            "I was ddos'd, my city was named, and I was kicked from a party by a person who wasn't the party leader, nor was he in the party.\n",
            "Hi, I have a question. Someone joined our party. 1. He said my city, st\n",
            "antiMLM: 215\n",
            "HydroHomies: 192\n",
            "total size: 444\n",
            "counts:\n",
            "[('water', 409), ('mlm', 288), ('like', 212), ('drink', 209), ('know', 191)]\n",
            "\n",
            "\n",
            "Cluster 5\n",
            "Example text:\n",
            "Just got my Xbox series s, haven't had any type of Xbox since the 360 so I'm new and have a question.\n",
            "I'm pretty sure I can but just to be sure. Can I make an account before I even turn the system on?\n",
            "xbox: 161\n",
            "NintendoSwitch: 46\n",
            "pcgaming: 47\n",
            "Coffee: 17\n",
            "PS4: 50\n",
            "total size: 323\n",
            "counts:\n",
            "[('xbox', 265), ('series', 178), ('controller', 131), ('x', 124), ('game', 104)]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I convert the cluster data to a list for our predicted labels. I also had to convert our \"labels\" list to a numerical representation for the confusion_matrix function to work"
      ],
      "metadata": {
        "id": "qQ6B6DHCPUvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_predicted = []\n",
        "for i in range(len(texts)):\n",
        "  labels_predicted.append(closest_centroid_per_text[i][0])\n",
        "label_dictionary = {'NintendoSwitch':0,'tea':1,'PS4':2,'Coffee':3,'pcgaming':4,'antiMLM':5,'xbox':6,'HydroHomies':7,'Soda':8}\n",
        "labels_numerical = [label_dictionary[label] for label in labels]"
      ],
      "metadata": {
        "id": "ZcOOXxE3H60g"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def plotConfusionMatrix(labels_val, labels_predicted):\n",
        "  cm = confusion_matrix(labels_val, labels_predicted)\n",
        "  colour_map = sn.color_palette(\"Reds\", as_cmap=True)\n",
        "  ylabels = ['NintendoSwitch','tea','PS4','Coffee','pcgaming','antiMLM','xbox','HydroHomies','Soda']\n",
        "  xlabels = [1,2,3,4,5]\n",
        "  plot = sn.heatmap(cm[:,:-4], \n",
        "                    annot=True, # Put the numbers in\n",
        "                    annot_kws={\"size\": 14}, # Make the numbers bigger\n",
        "                    fmt='g', # Stop scientific notation\n",
        "                    cmap = colour_map, # Choose the colour palette\n",
        "                    cbar = False, # Don't include the colour bar\n",
        "                    xticklabels=xlabels, # Put in the X and Y labels\n",
        "                    yticklabels=ylabels)\n",
        "  plot.set(xlabel='Predicted', ylabel='Actual')\n",
        "  return plot\n",
        "\n",
        "plotConfusionMatrix(labels_numerical, labels_predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "eYe4OcNYImWd",
        "outputId": "c3684370-f76b-4738-e511-69f8abaf3eb5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='Predicted', ylabel='Actual'>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 - Comparing Classifiers"
      ],
      "metadata": {
        "id": "aoJBjSLoYuvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dummy Classifier with strategy=\"most_frequent\". Most frequent labels in training set are shown below:"
      ],
      "metadata": {
        "id": "Ucb21jSR23PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(labels_train).most_common()"
      ],
      "metadata": {
        "id": "G7N8sLC8Y1CN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07b6a402-45d1-4e68-9664-89fe31a44fdf"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NintendoSwitch', 156),\n",
              " ('tea', 149),\n",
              " ('PS4', 148),\n",
              " ('Coffee', 135),\n",
              " ('pcgaming', 134),\n",
              " ('antiMLM', 132),\n",
              " ('xbox', 125),\n",
              " ('HydroHomies', 117),\n",
              " ('Soda', 104)]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacyfy)\n",
        "texts_train_tfidf = vectorizer.fit_transform(texts_train)\n",
        "texts_val_tfidf = vectorizer.transform(texts_val)\n",
        "\n",
        "dummy_clf_mf = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf_mf.fit(texts_train_tfidf, labels_train)\n",
        "\n",
        "labels_train_predicted = dummy_clf_mf.predict(texts_train_tfidf)\n",
        "labels_val_predicted = dummy_clf_mf.predict(texts_val_tfidf)\n",
        "\n",
        "#report = classification_report(labels_val, labels_predicted)\n",
        "train_accuracy = accuracy_score(labels_train, labels_train_predicted)\n",
        "train_precision = precision_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_recall = recall_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_f1 = f1_score(labels_train, labels_train_predicted, average='macro')\n",
        "\n",
        "val_accuracy = accuracy_score(labels_val, labels_val_predicted)\n",
        "val_precision = precision_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_recall = recall_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_f1 = f1_score(labels_val, labels_val_predicted, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Training', 'Validation'],\n",
        "    'Accuracy': [train_accuracy, val_accuracy],\n",
        "    'Macro Precision': [train_precision, val_precision],\n",
        "    'Macro Recall': [train_recall, val_recall],\n",
        "    'Macro F1-score': [train_f1, val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "best_val.name = 'Best performance'\n",
        "df = df.append(best_val)\n",
        "\n",
        "#print table to console\n",
        "print(\"Dummy Classifier with strategy=\\\"most_frequent\\\"\")\n",
        "print(df.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW5nZyjO6hJN",
        "outputId": "b8bcf669-c028-4f85-cc04-1838e684fe75"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy Classifier with strategy=\"most_frequent\"\n",
            "                         Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0                 Training    0.130     0.014            0.111         0.026         \n",
            "1                 Validation  0.138     0.015            0.111         0.027         \n",
            "Best performance  Validation  0.138     0.015            0.111         0.027         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dummy Classifier with strategy=\"stratified\""
      ],
      "metadata": {
        "id": "z0EtkKXQLJgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=spacyfy)\n",
        "texts_train_tfidf = vectorizer.fit_transform(texts_train)\n",
        "texts_val_tfidf = vectorizer.transform(texts_val)\n",
        "\n",
        "dummy_clf_mf = DummyClassifier(strategy=\"stratified\")\n",
        "dummy_clf_mf.fit(texts_train_tfidf, labels_train)\n",
        "\n",
        "labels_train_predicted = dummy_clf_mf.predict(texts_train_tfidf)\n",
        "labels_val_predicted = dummy_clf_mf.predict(texts_val_tfidf)\n",
        "\n",
        "#report = classification_report(labels_val, labels_predicted)\n",
        "train_accuracy = accuracy_score(labels_train, labels_train_predicted)\n",
        "train_precision = precision_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_recall = recall_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_f1 = f1_score(labels_train, labels_train_predicted, average='macro')\n",
        "\n",
        "val_accuracy = accuracy_score(labels_val, labels_val_predicted)\n",
        "val_precision = precision_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_recall = recall_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_f1 = f1_score(labels_val, labels_val_predicted, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Training', 'Validation'],\n",
        "    'Accuracy': [train_accuracy, val_accuracy],\n",
        "    'Macro Precision': [train_precision, val_precision],\n",
        "    'Macro Recall': [train_recall, val_recall],\n",
        "    'Macro F1-score': [train_f1, val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "best_val.name = 'Best performance'\n",
        "df = df.append(best_val)\n",
        "\n",
        "#print table to console\n",
        "print(\"Dummy Classifier with strategy=\\\"stratified\\\"\")\n",
        "print(df.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPuuIpgwLFpq",
        "outputId": "aa8bd53e-9f37-4a56-8da0-9ab19b6a59ed"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy Classifier with strategy=\"stratified\"\n",
            "                         Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0                 Training    0.11      0.107            0.108         0.108         \n",
            "1                 Validation  0.15      0.148            0.151         0.148         \n",
            "Best performance  Validation  0.15      0.148            0.151         0.148         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogisticRegression with One-hot vectorization"
      ],
      "metadata": {
        "id": "cpPMbCBwNXe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "texts_train_oh = vectorizer.fit_transform(texts_train)\n",
        "\n",
        "texts_val_oh = vectorizer.transform(texts_val)\n",
        "\n",
        "clf_lr_oh = LogisticRegression()\n",
        "clf_lr_oh.fit(texts_train_oh, labels_train)\n",
        "\n",
        "labels_train_predicted = clf_lr_oh.predict(texts_train_oh)\n",
        "labels_val_predicted = clf_lr_oh.predict(texts_val_oh)\n",
        "\n",
        "#report = classification_report(labels_val, labels_predicted)\n",
        "train_accuracy = accuracy_score(labels_train, labels_train_predicted)\n",
        "train_precision = precision_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_recall = recall_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_f1 = f1_score(labels_train, labels_train_predicted, average='macro')\n",
        "\n",
        "val_accuracy = accuracy_score(labels_val, labels_val_predicted)\n",
        "val_precision = precision_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_recall = recall_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_f1 = f1_score(labels_val, labels_val_predicted, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Training', 'Validation'],\n",
        "    'Accuracy': [train_accuracy, val_accuracy],\n",
        "    'Macro Precision': [train_precision, val_precision],\n",
        "    'Macro Recall': [train_recall, val_recall],\n",
        "    'Macro F1-score': [train_f1, val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "best_val.name = 'Best performance'\n",
        "df = df.append(best_val)\n",
        "\n",
        "#print table to console\n",
        "print(\"LogisticRegression with One-hot vectorization\")\n",
        "print(df.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w10Y-bR8NXBj",
        "outputId": "e83210e4-3a03-4b51-c155-96f5dae2f292"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression with One-hot vectorization\n",
            "                         Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0                 Training    1.00      1.000            1.000         1.000         \n",
            "1                 Validation  0.78      0.794            0.785         0.783         \n",
            "Best performance  Training    1.00      1.000            1.000         1.000         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LogisticRegression with TF-IDF vectorization (default settings)"
      ],
      "metadata": {
        "id": "CqMLwNhvOWUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=spacyfy)\n",
        "texts_train_tfidf = vectorizer.fit_transform(texts_train)\n",
        "texts_val_tfidf = vectorizer.transform(texts_val)\n",
        "\n",
        "clf_lr_tfidf = LogisticRegression()\n",
        "clf_lr_tfidf.fit(texts_train_tfidf, labels_train)\n",
        "\n",
        "labels_train_predicted = clf_lr_tfidf.predict(texts_train_tfidf)\n",
        "labels_val_predicted_lrtfidf = clf_lr_tfidf.predict(texts_val_tfidf)\n",
        "\n",
        "#report = classification_report(labels_val, labels_predicted)\n",
        "train_accuracy = accuracy_score(labels_train, labels_train_predicted)\n",
        "train_precision = precision_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_recall = recall_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_f1 = f1_score(labels_train, labels_train_predicted, average='macro')\n",
        "\n",
        "val_accuracy = accuracy_score(labels_val, labels_val_predicted_lrtfidf)\n",
        "val_precision = precision_score(labels_val, labels_val_predicted_lrtfidf, average='macro')\n",
        "val_recall = recall_score(labels_val, labels_val_predicted_lrtfidf, average='macro')\n",
        "val_f1 = f1_score(labels_val, labels_val_predicted_lrtfidf, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Training', 'Validation'],\n",
        "    'Accuracy': [train_accuracy, val_accuracy],\n",
        "    'Macro Precision': [train_precision, val_precision],\n",
        "    'Macro Recall': [train_recall, val_recall],\n",
        "    'Macro F1-score': [train_f1, val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "best_val.name = 'Best performance'\n",
        "df = df.append(best_val)\n",
        "\n",
        "#print table to console\n",
        "print(\"LogisticRegression with TF-IDF vectorization (default settings)\")\n",
        "print(df.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVs-Mq8xOTxV",
        "outputId": "15ef5266-329c-4f85-c633-595512a39d5c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression with TF-IDF vectorization (default settings)\n",
            "                         Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0                 Training    0.988     0.988            0.987         0.988         \n",
            "1                 Validation  0.832     0.848            0.833         0.834         \n",
            "Best performance  Training    0.988     0.988            0.987         0.988         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC Classifier with One-hot vectorization (SVM with RBF kernel, default settings)"
      ],
      "metadata": {
        "id": "FWjl4swxWDE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "vectorizer = CountVectorizer()\n",
        "texts_train_svc = vectorizer.fit_transform(texts_train)\n",
        "texts_val_svc = vectorizer.transform(texts_val)\n",
        "\n",
        "clf_svc = SVC()\n",
        "clf_svc.fit(texts_train_svc, labels_train)\n",
        "\n",
        "labels_train_predicted = clf_svc.predict(texts_train_svc)\n",
        "labels_val_predicted = clf_svc.predict(texts_val_svc)\n",
        "\n",
        "#report = classification_report(labels_val, labels_predicted)\n",
        "train_accuracy = accuracy_score(labels_train, labels_train_predicted)\n",
        "train_precision = precision_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_recall = recall_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_f1 = f1_score(labels_train, labels_train_predicted, average='macro')\n",
        "\n",
        "val_accuracy = accuracy_score(labels_val, labels_val_predicted)\n",
        "val_precision = precision_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_recall = recall_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_f1 = f1_score(labels_val, labels_val_predicted, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Training', 'Validation'],\n",
        "    'Accuracy': [train_accuracy, val_accuracy],\n",
        "    'Macro Precision': [train_precision, val_precision],\n",
        "    'Macro Recall': [train_recall, val_recall],\n",
        "    'Macro F1-score': [train_f1, val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "best_val.name = 'Best performance'\n",
        "df = df.append(best_val)\n",
        "\n",
        "#print table to console\n",
        "print(\"SVC Classifier with One-hot vectorization (SVM with RBF kernel, default settings)\")\n",
        "print(df.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqftRHOGWGPQ",
        "outputId": "bc537ebf-2d01-442c-cb13-deabec58ac9b"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVC Classifier with One-hot vectorization (SVM with RBF kernel, default settings)\n",
            "                         Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0                 Training    0.901     0.930            0.900         0.908         \n",
            "1                 Validation  0.588     0.704            0.598         0.614         \n",
            "Best performance  Training    0.901     0.930            0.900         0.908         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifier with the best performance so far has been LogisticRegression with TF-IDF vectorization on default settings, with a Macro F1-Score of 0.834 on the validation set. Below is code to show a bar chart of F1 scores per class."
      ],
      "metadata": {
        "id": "jNDEo6dgyMrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the pastel colors were suggested by chatGPT\n",
        "f1_scores_lrtfidf = f1_score(labels_val, labels_val_predicted_lrtfidf, average=None)\n",
        "subreddits = ['NintendoSwitch','tea','PS4','Coffee','pcgaming','antiMLM','xbox','HydroHomies','Soda']\n",
        "pastel_colors = ['#F8B195', '#F67280', '#C06C84', '#6C5B7B', '#355C7D', '#8D8741', '#CDB380', '#FFE5B4', '#ECD5E3']\n",
        "\n",
        "plt.bar(subreddits, f1_scores_lrtfidf, color=pastel_colors)\n",
        "plt.title(\"F1 score per subreddit\")\n",
        "plt.xlabel(\"Subreddit\")\n",
        "plt.ylabel(\"F1 score\")\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "1fTvGc-GyHbD",
        "outputId": "9e456fed-fb6b-4872-c633-325ecc96255f"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The approach I myself came up with is shown below: Multinomial Naive Bayes with TF-IDF vectorization"
      ],
      "metadata": {
        "id": "xu4O5c4VqEXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacyfy)\n",
        "texts_train_tfidf = vectorizer.fit_transform(texts_train)\n",
        "texts_val_tfidf = vectorizer.transform(texts_val)\n",
        "\n",
        "clf_mnb_tfidf = MultinomialNB()\n",
        "clf_mnb_tfidf.fit(texts_train_tfidf, labels_train)\n",
        "\n",
        "labels_train_predicted = clf_mnb_tfidf.predict(texts_train_tfidf)\n",
        "labels_val_predicted = clf_mnb_tfidf.predict(texts_val_tfidf)\n",
        "\n",
        "#report = classification_report(labels_val, labels_predicted)\n",
        "train_accuracy = accuracy_score(labels_train, labels_train_predicted)\n",
        "train_precision = precision_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_recall = recall_score(labels_train, labels_train_predicted, average='macro')\n",
        "train_f1 = f1_score(labels_train, labels_train_predicted, average='macro')\n",
        "\n",
        "val_accuracy = accuracy_score(labels_val, labels_val_predicted)\n",
        "val_precision = precision_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_recall = recall_score(labels_val, labels_val_predicted, average='macro')\n",
        "val_f1 = f1_score(labels_val, labels_val_predicted, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Training', 'Validation'],\n",
        "    'Accuracy': [train_accuracy, val_accuracy],\n",
        "    'Macro Precision': [train_precision, val_precision],\n",
        "    'Macro Recall': [train_recall, val_recall],\n",
        "    'Macro F1-score': [train_f1, val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "best_val.name = 'Best performance'\n",
        "df = df.append(best_val)\n",
        "\n",
        "#print table to console\n",
        "print(\"Multinomial Naive Bayes with TF-IDF vectorization\")\n",
        "print(df.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1c03YbtqC4Z",
        "outputId": "eeacc36d-af30-4063-e779-f76e2ab321fa"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Naive Bayes with TF-IDF vectorization\n",
            "                         Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0                 Training    0.986     0.987            0.985         0.986         \n",
            "1                 Validation  0.782     0.832            0.777         0.779         \n",
            "Best performance  Training    0.986     0.987            0.985         0.986         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4 -  Parameter Tuning"
      ],
      "metadata": {
        "id": "4uCTYWj3Y4HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter Tuning of LogisticRegression with TF-IDF vectorisation for both the vectorizer and classifier on the validation set. Tuned Parameters:\n",
        "\n",
        "\n",
        "1.   Classifier - Regularisation C value (typical values might be powers of 10(from 10^-3 to 10^5)\n",
        "2.   Vectorizer - Parameters: sublinear_tf and max_features (vocabulary size) (in a range None to 50k)\n",
        "3.   My own parameter chosen for tuning: Vectorizer - max_df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qOOvdWz74kco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "#the following comments document values that I investigated during tuning\n",
        "#c_values = [10**i for i in range(-3, 6)]\n",
        "#c_values = [i for i in range(1,11)]\n",
        "#c_values = [1 + i*(3-1)/9 for i in range(10)]\n",
        "# param_grid = {\n",
        "#     'sublinear_tf': [True, False], \n",
        "#     'max_features': [None, 1000,5000,10000,20000,30000,40000,50000]\n",
        "#     }\n",
        "#'max_features': 10000, 'sublinear_tf': True\n",
        "# param_grid = {\n",
        "#     'max_df' : [0.1, 0.5, 0.75, 0.9, 1.0]\n",
        "# }\n",
        "param_grid = {\n",
        "    'max_df' : [0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "}\n",
        "#for c_value in c_values:\n",
        "for params in ParameterGrid(param_grid):\n",
        "  print(\"\\n\")\n",
        "  vectorizer = TfidfVectorizer(tokenizer=spacyfy, sublinear_tf = True, max_features = 10000, max_df = params[\"max_df\"])\n",
        "  texts_train_tfidf = vectorizer.fit_transform(texts_train)\n",
        "  texts_val_tfidf = vectorizer.transform(texts_val)\n",
        "\n",
        "  clf_lr_tfidf = LogisticRegression(C = 1.7)\n",
        "  clf_lr_tfidf.fit(texts_train_tfidf, labels_train)\n",
        "\n",
        "  labels_train_predicted = clf_lr_tfidf.predict(texts_train_tfidf)\n",
        "  labels_val_predicted_lrtfidf = clf_lr_tfidf.predict(texts_val_tfidf)\n",
        "\n",
        "  #report = classification_report(labels_val, labels_predicted)\n",
        "  train_accuracy = accuracy_score(labels_train, labels_train_predicted)\n",
        "  train_precision = precision_score(labels_train, labels_train_predicted, average='macro')\n",
        "  train_recall = recall_score(labels_train, labels_train_predicted, average='macro')\n",
        "  train_f1 = f1_score(labels_train, labels_train_predicted, average='macro')\n",
        "\n",
        "  val_accuracy = accuracy_score(labels_val, labels_val_predicted_lrtfidf)\n",
        "  val_precision = precision_score(labels_val, labels_val_predicted_lrtfidf, average='macro')\n",
        "  val_recall = recall_score(labels_val, labels_val_predicted_lrtfidf, average='macro')\n",
        "  val_f1 = f1_score(labels_val, labels_val_predicted_lrtfidf, average='macro')\n",
        "\n",
        "  #print evaluation metrics\n",
        "  #create table of evaluation metrics\n",
        "  df = pd.DataFrame({\n",
        "      'Set': ['Validation'],\n",
        "      'Accuracy': [val_accuracy],\n",
        "      'Macro Precision': [val_precision],\n",
        "      'Macro Recall': [val_recall],\n",
        "      'Macro F1-score': [val_f1]\n",
        "  })\n",
        "\n",
        "  #highlight row with best validation performance\n",
        "  best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "\n",
        "  #print table to console\n",
        "  print(\"LogisticRegression with TF-IDF vectorization \" + str(params))\n",
        "  print(df.round(8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ldq6PbU_5QJw",
        "outputId": "68b51f6d-b80c-414a-f352-e1d97631f0bf"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression with TF-IDF vectorization {'max_df': 0.1}\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.805     0.812999         0.804349      0.80375       \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression with TF-IDF vectorization {'max_df': 0.2}\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.8375    0.851405         0.839209      0.839419      \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression with TF-IDF vectorization {'max_df': 0.3}\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.8425    0.854945         0.842807      0.843489      \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression with TF-IDF vectorization {'max_df': 0.4}\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.84      0.851644         0.839633      0.840785      \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression with TF-IDF vectorization {'max_df': 0.5}\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.84      0.851644         0.839633      0.840785      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5 - Context vectors using BERT"
      ],
      "metadata": {
        "id": "8e_PbJilY7cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFmUN-I4EqCp",
        "outputId": "1d9bdfbb-568f-4dac-8cfd-4576f88a14f2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline('feature-extraction', model=\"roberta-base\")"
      ],
      "metadata": {
        "id": "yvBnNbLhZCly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3332af15-4f72-4969-bc22-38b803f063d0"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/pytorch_model.bin\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of RobertaModel were initialized from the model checkpoint at roberta-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As roberta-base has a maximum sequence length of 512 tokens, and our training set for eg. already contains texts up to 726 tokens long, we must use a function that cuts down strings. 420 is the maximum value that I have found, through trial and error, to let the expanded size of the tensor match the existing size (514) at non-singleton dimension 1."
      ],
      "metadata": {
        "id": "NUPuXZY7MYYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_max_length(text_set, max_length):\n",
        "  constrained_text_set = []\n",
        "  for text in text_set:\n",
        "    temp = text.split()\n",
        "    if len(temp) > max_length:\n",
        "      temp = temp[:max_length]\n",
        "    constrained_text = \" \".join(temp)\n",
        "    constrained_text_set.append(constrained_text)\n",
        "  return constrained_text_set\n",
        "\n",
        "texts_train_cnstd = ensure_max_length(texts_train, 35)\n",
        "texts_val_cnstd = ensure_max_length(texts_val, 35)\n",
        "texts_test_cnstd = ensure_max_length(texts_test, 35)"
      ],
      "metadata": {
        "id": "valDyA0rL1ZY"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xV_3KQn3_X21"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts_train_cnstd[3].split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K228CRCVPjvG",
        "outputId": "a1f8b572-dbd8-4197-9927-914dd69bd8be"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoded_texts_train = pipe(texts_train_cnstd, return_tensors='pt')\n",
        "# encoded_texts_val = pipe(texts_val_cnstd, return_tensors='pt')\n",
        "# encoded_texts_test = pipe(texts_test_cnstd, return_tensors='pt')"
      ],
      "metadata": {
        "id": "nnDG65PGL-8f"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "encoded_texts_train = []\n",
        "for text in tqdm(texts_train_cnstd):\n",
        "  output = pipe(text, max_length=512, return_tensors='pt')\n",
        "  encoded_texts_train.append(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxkHIV1ZOXOL",
        "outputId": "ae4f0ffb-9f60-4a3e-eb1e-9b8a540aae1c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1200/1200 [04:03<00:00,  4.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts_val = []\n",
        "for text in tqdm(texts_val_cnstd):\n",
        "  output = pipe(text, max_length=512, return_tensors='pt')\n",
        "  encoded_texts_val.append(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX1LfML7VAUW",
        "outputId": "54b14c22-789c-47ab-cd67-b7fe61e7872c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [01:22<00:00,  4.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts_test = []\n",
        "for text in tqdm(texts_test_cnstd):\n",
        "  output = pipe(text, max_length=512, return_tensors='pt')\n",
        "  encoded_texts_test.append(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzoRrXucVSVB",
        "outputId": "0718b681-a5ce-4766-87de-7bb2b803a772"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [01:16<00:00,  5.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_texts_train_st = [text[0][0].numpy() for text in encoded_texts_train]\n",
        "enc_texts_val_st = [text[0][0].numpy() for text in encoded_texts_val]\n",
        "enc_texts_test_st = [text[0][0].numpy() for text in encoded_texts_test]\n",
        "\n",
        "#enc_texts_train_st = [pipe(text)[0][0].numpy() for text in texts_train_cnstd]"
      ],
      "metadata": {
        "id": "qPfY9jiaYv_-"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enc_texts_train_st = [text[:,0] for text in encoded_texts_train]\n",
        "# enc_texts_val_st = [text[:,0] for text in encoded_texts_val]\n",
        "# enc_texts_test_st = [text[:,0] for text in encoded_texts_test]"
      ],
      "metadata": {
        "id": "cK4VnWtzwTod"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "clf_lr_cv = LogisticRegression(max_iter=1000)\n",
        "clf_lr_cv.fit(enc_texts_train_st, labels_train)\n",
        "\n",
        "labels_train_predicted = clf_lr_cv.predict(enc_texts_train_st)\n",
        "labels_val_predicted = clf_lr_cv.predict(enc_texts_val_st)\n",
        "\n",
        "\n",
        "#report = classification_report(labels_val, labels_predicted)\n",
        "parameters = [[labels_train, labels_train_predicted],[labels_val, labels_val_predicted]]\n",
        "results = []\n",
        "for parameter_set in parameters:\n",
        "  accuracy = accuracy_score(parameter_set[0], parameter_set[1])\n",
        "  precision = precision_score(parameter_set[0], parameter_set[1], average='macro')\n",
        "  recall = recall_score(parameter_set[0], parameter_set[1], average='macro')\n",
        "  f1 = f1_score(parameter_set[0], parameter_set[1], average='macro')\n",
        "\n",
        "  result_list = [accuracy, precision, recall, f1]\n",
        "  results.append(result_list)\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set':              ['Training', 'Validation'],\n",
        "    'Accuracy':         [results[0][0], results[1][0]],\n",
        "    'Macro Precision':  [results[0][1], results[1][1]],\n",
        "    'Macro Recall':     [results[0][2], results[1][2]],\n",
        "    'Macro F1-score':   [results[0][3], results[1][3]]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "best_val.name = 'Best performance'\n",
        "df = df.append(best_val)\n",
        "\n",
        "#print table to console\n",
        "print(\"LogisticRegression on the first encoded context vector per text using 'feature-extraction' and 'roberta-base'S\")\n",
        "print(df.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqjya64tRpH9",
        "outputId": "1ff58c01-3ab8-4f47-e7e7-824549dae686"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression on the first encoded context vector per text using 'feature-extraction' and 'roberta-base'S\n",
            "                         Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0                 Training    0.846     0.854            0.848         0.85          \n",
            "1                 Validation  0.778     0.773            0.771         0.77          \n",
            "Best performance  Training    0.846     0.854            0.848         0.85          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we train an end-to-end classifier using 'trainer'"
      ],
      "metadata": {
        "id": "Va_IIuNJd4Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "batch_size = 16\n",
        "epochs = 1 \n",
        "weight_decay = 0.0"
      ],
      "metadata": {
        "id": "ys7yzM0Ld3zQ"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\", # HuggingFace wants a name for your model\n",
        "    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n",
        "    learning_rate=learning_rate, # Hyperparameter\n",
        "    per_device_train_batch_size=batch_size, # Hyperparameter\n",
        "    per_device_eval_batch_size=batch_size, # Hyperparameter\n",
        "    num_train_epochs=epochs, # Hyperparameter\n",
        "    weight_decay=weight_decay, # Hyperparameter\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NbS3YeKeEym",
        "outputId": "c12a350c-47a5-47ed-be99-bbb8049783ea"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eti_eT9eVFY",
        "outputId": "54ca9b71-c128-45cf-ea20-affaf9eefc50"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"roberta-base\", num_labels=9)\n",
        "model = AutoModelForSequenceClassification.from_config(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL1aYeHTexEA",
        "outputId": "f7c361c2-842e-4d75-9d0a-9c4cb81f5783"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "labels = ['NintendoSwitch','tea','PS4','Coffee','pcgaming','antiMLM','xbox','HydroHomies','Soda']\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels_train = label_encoder.fit_transform(labels_train)\n",
        "encoded_labels_val = label_encoder.fit_transform(labels_val)\n",
        "encoded_labels_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndSda8v9g8Yk",
        "outputId": "5596c2c4-c3b4-4657-fa15-8cd0c8e837e4"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 0, 5, ..., 7, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts_train_for_input_ids = tokenizer(texts_train_cnstd, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "encoded_texts_val_for_input_ids = tokenizer(texts_val_cnstd, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "input_ids_train = encoded_texts_train_for_input_ids[\"input_ids\"]\n",
        "input_ids_val = encoded_texts_val_for_input_ids[\"input_ids\"]"
      ],
      "metadata": {
        "id": "dJe4qEG2nKua"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_ids_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2Oy47VjpUCR",
        "outputId": "7cf9be98-bdda-4b34-9ab6-eff0386bd093"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0, 10975,  2709,  ...,     1,     1,     1],\n",
            "        [    0,   387, 12807,  ...,     1,     1,     1],\n",
            "        [    0, 28062,  6228,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    0, 29042, 25440,  ...,     1,     1,     1],\n",
            "        [    0,   100,   437,  ...,     1,     1,     1],\n",
            "        [    0, 42968, 17132,  ...,     1,     1,     1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_dict = {\"input_ids\": input_ids_train, \"labels\": encoded_labels_train}\n",
        "eval_dataset_dict = {\"input_ids\": input_ids_val, \"labels\": encoded_labels_val}\n",
        "print(len(eval_dataset_dict[\"labels\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgJ8lfT4gSVw",
        "outputId": "a43471bc-f448-41c6-ae16-4e5b66671107"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_dict(train_dataset_dict)\n",
        "eval_dataset = Dataset.from_dict(eval_dataset_dict)"
      ],
      "metadata": {
        "id": "T2wLigePk1Id"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZO5cBPpxLXd",
        "outputId": "3d7f7428-eb29-4c12-b4f4-852ba5b36369"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'labels'],\n",
            "    num_rows: 400\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, # The model you want to train\n",
        "    args=training_args, # The various training arguments set up above\n",
        "    train_dataset=train_dataset, # The data to use to update the weights\n",
        "    eval_dataset=eval_dataset, # The data to use for evaluation\n",
        "    tokenizer=tokenizer, # The tokenizer used on the data\n",
        "    data_collator=data_collator, # A data collator that does clever things moving data around\n",
        ")"
      ],
      "metadata": {
        "id": "qtv0pdqfemWr"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "ov8ayX8rflIQ",
        "outputId": "dfc28b1f-37b2-4088-9694-46a60e833a3f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1200\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 75\n",
            "  Number of trainable parameters = 124652553\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [75/75 00:24, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.195134</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=75, training_loss=2.2799652099609373, metrics={'train_runtime': 25.1725, 'train_samples_per_second': 47.671, 'train_steps_per_second': 2.979, 'total_flos': 64137350534400.0, 'train_loss': 2.2799652099609373, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, label_ids, metrics = trainer.predict(eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "u6B8SHkMgyDf",
        "outputId": "b15169eb-9420-46d2-8515-6ab91aab364f"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 400\n",
            "  Batch size = 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA4htvCc1E6k",
        "outputId": "0d4497f5-6623-4bfa-cafa-33120273b500"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.11704837  0.31202012  0.271253    0.0320636  -0.14343163  0.04907508\n",
            "  0.30272326  0.01425607  0.00043283]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmfNh772yWs6",
        "outputId": "9d127947-15b6-474a-e9d7-592d7dfd784d"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 2.195133686065674,\n",
              " 'test_runtime': 2.3709,\n",
              " 'test_samples_per_second': 168.713,\n",
              " 'test_steps_per_second': 10.545}"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_labels = []\n",
        "for prediction in predictions:\n",
        "  subreddit_index = np.argmax(prediction)\n",
        "  subreddit_label = label_encoder.inverse_transform([subreddit_index])[0]\n",
        "  predicted_labels.append(subreddit_label)"
      ],
      "metadata": {
        "id": "hvAGemw72MAs"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicted_labels\n",
        "Counter(labels_val).most_common()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqIk9Kf03PsW",
        "outputId": "301a346b-8dda-4047-d7f4-d44c5d2b891f"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NintendoSwitch', 55),\n",
              " ('pcgaming', 51),\n",
              " ('antiMLM', 47),\n",
              " ('tea', 46),\n",
              " ('HydroHomies', 45),\n",
              " ('xbox', 43),\n",
              " ('Coffee', 40),\n",
              " ('Soda', 38),\n",
              " ('PS4', 35)]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wVm62Gjy21JA"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "val_accuracy = accuracy_score(labels_val, predicted_labels)\n",
        "val_precision = precision_score(labels_val, predicted_labels, average='macro')\n",
        "val_recall = recall_score(labels_val, predicted_labels, average='macro')\n",
        "val_f1 = f1_score(labels_val, predicted_labels, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Validation'],\n",
        "    'Accuracy': [val_accuracy],\n",
        "    'Macro Precision': [val_precision],\n",
        "    'Macro Recall': [val_recall],\n",
        "    'Macro F1-score': [val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "\n",
        "#print table to console\n",
        "print(\"End-to-end classifier using trainer on roberta-base, with learning rate = 1e-4, epochs = 1, batch_size = 16 and no weight decay\")\n",
        "print(df.round(8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlSMfZsh0hhx",
        "outputId": "8578abbc-e638-4f18-99e9-6b5bb704f5b8"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End-to-end classifier using trainer on roberta-base, with learning rate = 1e-4, epochs = 1, batch_size = 16 and no weight decay\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.1125    0.0125           0.111111      0.022472      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5c): Different sets of hyperparameters\n",
        "Set 1:\n",
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "weight_decay = 1e-5\n",
        "\n",
        "Set 2:\n",
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "weight_decay = 1e-4"
      ],
      "metadata": {
        "id": "PdZf0vX-5WBx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "daTqWtH_2ZhH"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "weight_decay = 1e-5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\", # HuggingFace wants a name for your model\n",
        "    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n",
        "    learning_rate=learning_rate, # Hyperparameter\n",
        "    per_device_train_batch_size=batch_size, # Hyperparameter\n",
        "    per_device_eval_batch_size=batch_size, # Hyperparameter\n",
        "    num_train_epochs=epochs, # Hyperparameter\n",
        "    weight_decay=weight_decay, # Hyperparameter\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"roberta-base\", num_labels=9)\n",
        "model = AutoModelForSequenceClassification.from_config(config)\n",
        "\n",
        "labels = ['NintendoSwitch','tea','PS4','Coffee','pcgaming','antiMLM','xbox','HydroHomies','Soda']\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels_train = label_encoder.fit_transform(labels_train)\n",
        "encoded_labels_val = label_encoder.fit_transform(labels_val)\n",
        "encoded_labels_train\n",
        "\n",
        "encoded_texts_train_for_input_ids = tokenizer(texts_train_cnstd, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "encoded_texts_val_for_input_ids = tokenizer(texts_val_cnstd, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "input_ids_train = encoded_texts_train_for_input_ids[\"input_ids\"]\n",
        "input_ids_val = encoded_texts_val_for_input_ids[\"input_ids\"]\n",
        "\n",
        "train_dataset_dict = {\"input_ids\": input_ids_train, \"labels\": encoded_labels_train}\n",
        "eval_dataset_dict = {\"input_ids\": input_ids_val, \"labels\": encoded_labels_val}\n",
        "print(len(eval_dataset_dict[\"labels\"]))\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_dataset_dict)\n",
        "eval_dataset = Dataset.from_dict(eval_dataset_dict)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "#print(data_collator)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, # The model you want to train\n",
        "    args=training_args, # The various training arguments set up above\n",
        "    train_dataset=train_dataset, # The data to use to update the weights\n",
        "    eval_dataset=eval_dataset, # The data to use for evaluation\n",
        "    tokenizer=tokenizer, # The tokenizer used on the data\n",
        "    data_collator=data_collator, # A data collator that does clever things moving data around\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9Vwuo_u5hFo",
        "outputId": "5ad87c24-1461-4f36-d66f-8ed490000e83"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-qcouiFD5luH",
        "outputId": "2ee7ad42-4f48-4bc2-be1f-4f63589aa516"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1200\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 380\n",
            "  Number of trainable parameters = 124652553\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='380' max='380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [380/380 03:47, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.256530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.239929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.283920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.236473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.265020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.234593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.225774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.204735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.213750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.202548</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=380, training_loss=2.287077572471217, metrics={'train_runtime': 228.0923, 'train_samples_per_second': 52.61, 'train_steps_per_second': 1.666, 'total_flos': 641373505344000.0, 'train_loss': 2.287077572471217, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, label_ids, metrics = trainer.predict(eval_dataset)\n",
        "predicted_labels = []\n",
        "for prediction in predictions:\n",
        "  subreddit_index = np.argmax(prediction)\n",
        "  subreddit_label = label_encoder.inverse_transform([subreddit_index])[0]\n",
        "  predicted_labels.append(subreddit_label)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "val_accuracy = accuracy_score(labels_val, predicted_labels)\n",
        "val_precision = precision_score(labels_val, predicted_labels, average='macro')\n",
        "val_recall = recall_score(labels_val, predicted_labels, average='macro')\n",
        "val_f1 = f1_score(labels_val, predicted_labels, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Validation'],\n",
        "    'Accuracy': [val_accuracy],\n",
        "    'Macro Precision': [val_precision],\n",
        "    'Macro Recall': [val_recall],\n",
        "    'Macro F1-score': [val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "\n",
        "#print table to console\n",
        "print(\"End-to-end classifier using trainer on roberta-base, with learning_rate = 1e-3, Sbatch_size = 32, epochs = 10, weight_decay = 1e-5\")\n",
        "print(df.round(8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "uYh_clxs9IXQ",
        "outputId": "33329f13-28f3-4511-b23d-062d8d7b6116"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End-to-end classifier using trainer on roberta-base, with learning_rate = 1e-3, Sbatch_size = 32, epochs = 10, weight_decay = 1e-5\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.1375    0.015278         0.111111      0.026862      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second set of hyperparameters: \n",
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "weight_decay = 1e-4"
      ],
      "metadata": {
        "id": "8ulwDngJAhaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "weight_decay = 1e-4\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\", # HuggingFace wants a name for your model\n",
        "    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n",
        "    learning_rate=learning_rate, # Hyperparameter\n",
        "    per_device_train_batch_size=batch_size, # Hyperparameter\n",
        "    per_device_eval_batch_size=batch_size, # Hyperparameter\n",
        "    num_train_epochs=epochs, # Hyperparameter\n",
        "    weight_decay=weight_decay, # Hyperparameter\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"roberta-base\", num_labels=9)\n",
        "model = AutoModelForSequenceClassification.from_config(config)\n",
        "\n",
        "labels = ['NintendoSwitch','tea','PS4','Coffee','pcgaming','antiMLM','xbox','HydroHomies','Soda']\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels_train = label_encoder.fit_transform(labels_train)\n",
        "encoded_labels_val = label_encoder.fit_transform(labels_val)\n",
        "encoded_labels_train\n",
        "\n",
        "encoded_texts_train_for_input_ids = tokenizer(texts_train_cnstd, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "encoded_texts_val_for_input_ids = tokenizer(texts_val_cnstd, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "input_ids_train = encoded_texts_train_for_input_ids[\"input_ids\"]\n",
        "input_ids_val = encoded_texts_val_for_input_ids[\"input_ids\"]\n",
        "\n",
        "train_dataset_dict = {\"input_ids\": input_ids_train, \"labels\": encoded_labels_train}\n",
        "eval_dataset_dict = {\"input_ids\": input_ids_val, \"labels\": encoded_labels_val}\n",
        "print(len(eval_dataset_dict[\"labels\"]))\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_dataset_dict)\n",
        "eval_dataset = Dataset.from_dict(eval_dataset_dict)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "#print(data_collator)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, # The model you want to train\n",
        "    args=training_args, # The various training arguments set up above\n",
        "    train_dataset=train_dataset, # The data to use to update the weights\n",
        "    eval_dataset=eval_dataset, # The data to use for evaluation\n",
        "    tokenizer=tokenizer, # The tokenizer used on the data\n",
        "    data_collator=data_collator, # A data collator that does clever things moving data around\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoG0i99j9uoc",
        "outputId": "21ff3c87-a401-4eb8-f39e-cc29dfd175a4"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R-YG0w48Au54",
        "outputId": "8ba74b08-dc8e-4f16-f858-f8c76035b12f"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1200\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 64\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1900\n",
            "  Number of trainable parameters = 124652553\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1900' max='1900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1900/1900 35:53, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>4.187460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.705859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.750918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.583448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.555523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.917461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.647347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.786024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.564052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.515482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.593020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.743698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.789895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.475546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.657399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.593149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.029506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.289993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.510277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.309910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.746079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.700967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.492039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.391857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.868735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.377404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.985590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.767464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.343056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.579061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.496731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.399870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.601131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.446007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.330079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.264903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.453435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.514994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.984345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.594836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.697580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.589786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.512742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.348850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.444330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.668741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.593713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.441749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.313279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.430153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.453849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.732700</td>\n",
              "      <td>2.351568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.610953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.330621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.349218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.615283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.293468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.473203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.524378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.379577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.379644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.294521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.451755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.434864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.354478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.292189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.454041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.372475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.300501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.352436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.423634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.393658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.408892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.377622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.261712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.333027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.267321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.506200</td>\n",
              "      <td>2.223341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.234916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.354812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.304757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.262021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.244861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.246793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.232028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.233677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.282063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.219754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.262156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.246267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.209065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.213451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.244420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.227563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.241825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.215844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.199143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.205837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.199275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.388900</td>\n",
              "      <td>2.199205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to my_awesome_model/checkpoint-500\n",
            "Configuration saved in my_awesome_model/checkpoint-500/config.json\n",
            "Model weights saved in my_awesome_model/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in my_awesome_model/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in my_awesome_model/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to my_awesome_model/checkpoint-1000\n",
            "Configuration saved in my_awesome_model/checkpoint-1000/config.json\n",
            "Model weights saved in my_awesome_model/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in my_awesome_model/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in my_awesome_model/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to my_awesome_model/checkpoint-1500\n",
            "Configuration saved in my_awesome_model/checkpoint-1500/config.json\n",
            "Model weights saved in my_awesome_model/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in my_awesome_model/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in my_awesome_model/checkpoint-1500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1900, training_loss=2.484320132606908, metrics={'train_runtime': 2154.2145, 'train_samples_per_second': 55.705, 'train_steps_per_second': 0.882, 'total_flos': 6413735053440000.0, 'train_loss': 2.484320132606908, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, label_ids, metrics = trainer.predict(eval_dataset)\n",
        "predicted_labels = []\n",
        "for prediction in predictions:\n",
        "  subreddit_index = np.argmax(prediction)\n",
        "  subreddit_label = label_encoder.inverse_transform([subreddit_index])[0]\n",
        "  predicted_labels.append(subreddit_label)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "val_accuracy = accuracy_score(labels_val, predicted_labels)\n",
        "val_precision = precision_score(labels_val, predicted_labels, average='macro')\n",
        "val_recall = recall_score(labels_val, predicted_labels, average='macro')\n",
        "val_f1 = f1_score(labels_val, predicted_labels, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Validation'],\n",
        "    'Accuracy': [val_accuracy],\n",
        "    'Macro Precision': [val_precision],\n",
        "    'Macro Recall': [val_recall],\n",
        "    'Macro F1-score': [val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "\n",
        "#print table to console\n",
        "print(\"End-to-end classifier using trainer on roberta-base, with learning_rate = 1e-2, Sbatch_size = 64, epochs = 100, weight_decay = 1e-4\")\n",
        "print(df.round(8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "tF2mrJD5A18b",
        "outputId": "b731e129-6337-4f24-b339-45031daeb350"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 400\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End-to-end classifier using trainer on roberta-base, with learning_rate = 1e-2, Sbatch_size = 64, epochs = 100, weight_decay = 1e-4\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.1375    0.015278         0.111111      0.026862      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3rd set if parameters:\n"
      ],
      "metadata": {
        "id": "eETQPpFT2aNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5\n",
        "batch_size = 32\n",
        "epochs = 25\n",
        "weight_decay = 1e-2\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\", # HuggingFace wants a name for your model\n",
        "    evaluation_strategy=\"epoch\", # How often we want to evaluate the model\n",
        "    learning_rate=learning_rate, # Hyperparameter\n",
        "    per_device_train_batch_size=batch_size, # Hyperparameter\n",
        "    per_device_eval_batch_size=batch_size, # Hyperparameter\n",
        "    num_train_epochs=epochs, # Hyperparameter\n",
        "    weight_decay=weight_decay, # Hyperparameter\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"roberta-base\", num_labels=9)\n",
        "model = AutoModelForSequenceClassification.from_config(config)\n",
        "\n",
        "labels = ['NintendoSwitch','tea','PS4','Coffee','pcgaming','antiMLM','xbox','HydroHomies','Soda']\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels_train = label_encoder.fit_transform(labels_train)\n",
        "encoded_labels_val = label_encoder.fit_transform(labels_val)\n",
        "encoded_labels_train\n",
        "\n",
        "encoded_texts_train_for_input_ids = tokenizer(texts_train_cnstd, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "encoded_texts_val_for_input_ids = tokenizer(texts_val_cnstd, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "input_ids_train = encoded_texts_train_for_input_ids[\"input_ids\"]\n",
        "input_ids_val = encoded_texts_val_for_input_ids[\"input_ids\"]\n",
        "\n",
        "train_dataset_dict = {\"input_ids\": input_ids_train, \"labels\": encoded_labels_train}\n",
        "eval_dataset_dict = {\"input_ids\": input_ids_val, \"labels\": encoded_labels_val}\n",
        "print(len(eval_dataset_dict[\"labels\"]))\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_dataset_dict)\n",
        "eval_dataset = Dataset.from_dict(eval_dataset_dict)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "#print(data_collator)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, # The model you want to train\n",
        "    args=training_args, # The various training arguments set up above\n",
        "    train_dataset=train_dataset, # The data to use to update the weights\n",
        "    eval_dataset=eval_dataset, # The data to use for evaluation\n",
        "    tokenizer=tokenizer, # The tokenizer used on the data\n",
        "    data_collator=data_collator, # A data collator that does clever things moving data around\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCuKj9pf2dbv",
        "outputId": "11a1ae48-3038-4476-9e2a-be64d6454731"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/merges.txt\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uqt2BU3Z3eCp",
        "outputId": "06d04bdb-daea-4da4-a6f6-7d03f03cfa8a"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 1200\n",
            "  Num Epochs = 25\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 950\n",
            "  Number of trainable parameters = 124652553\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [950/950 09:41, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.202411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.193274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.205780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.217999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.199453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.199346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.193736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.196563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.204711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.180887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.002385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.966779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.847558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.803312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.673889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.760973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.628979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.534972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.531210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.449478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.428873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.420663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.413813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.409612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.155000</td>\n",
              "      <td>1.399868</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "Saving model checkpoint to my_awesome_model/checkpoint-500\n",
            "Configuration saved in my_awesome_model/checkpoint-500/config.json\n",
            "Model weights saved in my_awesome_model/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in my_awesome_model/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in my_awesome_model/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=950, training_loss=1.7408611096833881, metrics={'train_runtime': 582.0337, 'train_samples_per_second': 51.543, 'train_steps_per_second': 1.632, 'total_flos': 1603433763360000.0, 'train_loss': 1.7408611096833881, 'epoch': 25.0})"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, label_ids, metrics = trainer.predict(eval_dataset)\n",
        "predicted_labels = []\n",
        "for prediction in predictions:\n",
        "  subreddit_index = np.argmax(prediction)\n",
        "  subreddit_label = label_encoder.inverse_transform([subreddit_index])[0]\n",
        "  predicted_labels.append(subreddit_label)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "val_accuracy = accuracy_score(labels_val, predicted_labels)\n",
        "val_precision = precision_score(labels_val, predicted_labels, average='macro')\n",
        "val_recall = recall_score(labels_val, predicted_labels, average='macro')\n",
        "val_f1 = f1_score(labels_val, predicted_labels, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Validation'],\n",
        "    'Accuracy': [val_accuracy],\n",
        "    'Macro Precision': [val_precision],\n",
        "    'Macro Recall': [val_recall],\n",
        "    'Macro F1-score': [val_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "\n",
        "#print table to console\n",
        "print(\"End-to-end classifier using trainer on roberta-base, with learning_rate = 1e-5 batch_size = 32 epochs = 25 weight_decay = 1e-2\")\n",
        "print(df.round(8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "2Gz0nQ8E3kjf",
        "outputId": "faa6ad11-941c-4f24-a6e4-12ae0e8d16c1"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 400\n",
            "  Batch size = 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End-to-end classifier using trainer on roberta-base, with learning_rate = 1e-5 batch_size = 32 epochs = 25 weight_decay = 1e-2\n",
            "          Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Validation  0.5625    0.579639         0.561818      0.563639      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q 6 - Conclusions and Future Work"
      ],
      "metadata": {
        "id": "WiktcpWFBY00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacyfy, sublinear_tf = True, max_features = 10000, max_df = 0.3)\n",
        "texts_train_tfidf = vectorizer.fit_transform(texts_train)\n",
        "texts_test_tfidf = vectorizer.transform(texts_test)\n",
        "\n",
        "clf_lr_tfidf = LogisticRegression(C = 1.7)\n",
        "clf_lr_tfidf.fit(texts_train_tfidf, labels_train)\n",
        "\n",
        "labels_train_predicted = clf_lr_tfidf.predict(texts_train_tfidf)\n",
        "labels_test_predicted_lrtfidf = clf_lr_tfidf.predict(texts_test_tfidf)\n",
        "\n",
        "#report = classification_report(labels_val, labels_predicted)\n",
        "\n",
        "test_accuracy = accuracy_score(labels_test, labels_test_predicted_lrtfidf)\n",
        "test_precision = precision_score(labels_test, labels_test_predicted_lrtfidf, average='macro')\n",
        "test_recall = recall_score(labels_test, labels_test_predicted_lrtfidf, average='macro')\n",
        "test_f1 = f1_score(labels_test, labels_test_predicted_lrtfidf, average='macro')\n",
        "\n",
        "#print evaluation metrics\n",
        "#create table of evaluation metrics\n",
        "df = pd.DataFrame({\n",
        "    'Set': ['Test'],\n",
        "    'Accuracy': [test_accuracy],\n",
        "    'Macro Precision': [test_precision],\n",
        "    'Macro Recall': [test_recall],\n",
        "    'Macro F1-score': [test_f1]\n",
        "})\n",
        "\n",
        "#highlight row with best validation performance\n",
        "best_val = df.iloc[df['Macro F1-score'].idxmax()]\n",
        "\n",
        "#print table to console\n",
        "print(\"LogisticRegression with TF-IDF vectorization on the test set with sublinear_tf = True, max_features = 10000, max_df = 0.3, and C = 1.7\")\n",
        "print(df.round(8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP8eN5GaBbi4",
        "outputId": "554050c4-121c-40cf-a139-2e309686ddaf"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression with TF-IDF vectorization on the test set with sublinear_tf = True, max_features = 10000, max_df = 0.3, and C = 1.7\n",
            "    Set  Accuracy  Macro Precision  Macro Recall  Macro F1-score\n",
            "0  Test  0.8675    0.869354         0.869186      0.868125      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels_test_predicted_lrtfidf[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQvb8zG2FYdZ",
        "outputId": "ee9e1edb-9512-4aaf-8ca3-616478525f25"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_predicted = []\n",
        "for i in range(len(texts)):\n",
        "  labels_predicted.append(closest_centroid_per_text[i][0])\n",
        "label_dictionary = {'NintendoSwitch':0,'tea':1,'PS4':2,'Coffee':3,'pcgaming':4,'antiMLM':5,'xbox':6,'HydroHomies':7,'Soda':8}\n",
        "labels_numerical = [label_dictionary[label] for label in labels]"
      ],
      "metadata": {
        "id": "Xkrk4DifFKgQ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def plotConfusionMatrix(labels_val, labels_predicted):\n",
        "  cm = confusion_matrix(labels_val, labels_predicted)\n",
        "  colour_map = sn.color_palette(\"Reds\", as_cmap=True)\n",
        "  ylabels = ['NintendoSwitch','tea','PS4','Coffee','pcgaming','antiMLM','xbox','HydroHomies','Soda']\n",
        "  xlabels = ['NintendoSwitch','tea','PS4','Coffee','pcgaming','antiMLM','xbox','HydroHomies','Soda']\n",
        "  plot = sn.heatmap(cm[:,:], \n",
        "                    annot=True, # Put the numbers in\n",
        "                    annot_kws={\"size\": 14}, # Make the numbers bigger\n",
        "                    fmt='g', # Stop scientific notation\n",
        "                    cmap = colour_map, # Choose the colour palette\n",
        "                    cbar = False, # Don't include the colour bar\n",
        "                    xticklabels=xlabels, # Put in the X and Y labels\n",
        "                    yticklabels=ylabels)\n",
        "  plot.set(xlabel='Predicted', ylabel='Actual')\n",
        "  return plot\n",
        "\n",
        "plotConfusionMatrix(labels_test, labels_test_predicted_lrtfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "Sz2WicgZFOOn",
        "outputId": "53afc166-cfae-4fde-a481-717bb1106d44"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:xlabel='Predicted', ylabel='Actual'>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zipped = zip(labels_test, labels_test_predicted_lrtfidf)\n",
        "zlist = list(zipped)"
      ],
      "metadata": {
        "id": "NFunckXsJSMy"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fn_coffee = []\n",
        "for i in range(len(zlist)):\n",
        "  if (zlist[i][0] == \"Coffee\") & (zlist[i][1] != \"Coffee\"):\n",
        "    fn_coffee.append(i)"
      ],
      "metadata": {
        "id": "GF6ViBaeJz5Z"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fn_coffee)\n",
        "for ix in fn_coffee:\n",
        "  print(zlist[ix][1])\n",
        "  print(texts_test[ix])\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_OH_GydLFEX",
        "outputId": "63de6210-d380-4a2b-fd39-6d6d30438cf3"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[106, 155, 174, 275, 345]\n",
            "HydroHomies\n",
            "What drink is the best judge of a cafe?\n",
            "Obviously answers will vary, but what is your go to drink when trying a new cafe for the first time to see how it stands up? This barista I know says a flat white, so was interested to see what others thought.\n",
            "\n",
            "\n",
            "PS4\n",
            "Huge shoutout to /u/suareasy! Sent a free Bodum carafe my way, thanks!\n",
            "I just wanted to give a public thanks to /u/suareasy for his generosity. He made a post last week offering a French Press carafe and was kind enough to send it my way, no strings attached. \n",
            "Pic of carafe next to my Bodum Chambord frame and plunger\n",
            "Thank you again!\n",
            "\n",
            "\n",
            "NintendoSwitch\n",
            "What's the one third wave shop you'd recommend visiting for a two-day trip to Anaheim?\n",
            "Willing to Uber to any shop outside of Anaheim, as there doesn't appear to be anything worthwhile around the area. What's the one shop you'd recommend me checking out in the surrounding area? The one that I just need to go to. \n",
            "I have been to Go Get 'Em Tiger when I was in LA a few years ago.\n",
            "\n",
            "\n",
            "pcgaming\n",
            "BES870XL problem\n",
            "My BES870XL has a leaking problem when it tries to make a shot. What parts do I have to replace? Below is the video.\n",
            "https://reddit.com/link/ca7z9c/video/c9lesvwofw831/player\n",
            "​\n",
            "​\n",
            "https://preview.redd.it/blln3plmlz831.jpg?width=1608&format=pjpg&auto=webp&s=e896f327de8ef788b7dc61250aea8f91eb4a0896\n",
            "It seems that the part inside red circle is leaking.\n",
            "\n",
            "\n",
            "PS4\n",
            "Induction frothers: an endangered species? O_o\n",
            "Has anyone else noticed that induction-based milk frothers + heaters are becoming harder to find? Googling \"induction frother\", the two top hits (both on Amazon) are \"currently unavailable\":\n",
            "Eurolux Pro XL EL-4000: http://amzn.com/B01MG5KEN3\n",
            "JAVA Group's Automatic Electric Milk Frother: http://amzn.com/B01MR8CE6I\n",
            "What gives? Are they starting fires? Patent problems? Are they ever coming back?! I own a Eurolux, and I'm terrified of the day it breaks down -- we need answers!\n",
            "PS: People who point to these $5 \"wand frothers\" have surely never had the insanely smooth frothy goodness that comes from an induction frother. I'm talking about milk foam that you can eat with a spoon. Bubbles so fine you don't see bubbles. Froth so good it could start a war, people! Feast your eyes: http://ibb.co/h154Sp\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}